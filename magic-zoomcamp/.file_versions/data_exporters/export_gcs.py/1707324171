from mage_ai.settings.repo import get_repo_path
from mage_ai.io.config import ConfigFileLoader, ConfigKey
from pandas import DataFrame
import os
import pyarrow as pa

if 'data_exporter' not in globals():
    from mage_ai.data_preparation.decorators import data_exporter


@data_exporter
def export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:
    """
    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage
    """
    # get the gcp credentials path from mage config file.
    config_path = os.path.join(get_repo_path(), 'io_config.yaml')
    config_profile = 'dev'
    config = ConfigFileLoader(config_path, config_profile)
    gsa_acc_key = config.get(ConfigKey.GOOGLE_SERVICE_ACC_KEY_FILEPATH)

    # Set the GCS application credentials environment variable
    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = gsa_acc_key

    project_id = 'de-zoomcamp-411619'
    bucket_name = 'mage-zoomcamp-kris-sert'
    table_name = f'nyc_green_taxi_data'
    # partition_column = 'lpep_pickup_date'
    root_path = f'{bucket_name}/{table_name}' 

    # Create GCSFileSystem
    gcs_fs = pa.fs.GcsFileSystem()

    pa.parquet.write_table(
        table=pa.Table.from_pandas(df),
        where=f'{root_path}/{file_name}.parquet',  # Specify the exact file name
        filesystem=gcs_fs,
        compression='snappy'  # Optional: specify the compression algorithm
    )

    # Write DataFrame to partitioned Parquet files in GCS
    pa.parquet.write_to_dataset(
        table=pa.Table.from_pandas(df),
        where=f'{root_path}/green_taxi_data_2022.parquet',
        base_path=root_path,
        # partition_cols=[partition_column],
        filesystem=gcs_fs,
        compression='snappy'  # Optional: specify the compression algorithm
    )